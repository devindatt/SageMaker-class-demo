{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standalone Bank Offer Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Customer Enrollment Dataset with Amazon SageMaker \n",
    "\n",
    "### Background\n",
    "\n",
    "Direct marketing, either through mail, email, phone, etc., is a common tactic to acquire customers. Because resources and a customer's attention is limited, the goal is to only target the subset of prospects who are likely to engage with a specific offer. Predicting those potential customers based on readily available information like demographics, past interactions, and environmental factors is a common machine learning problem.\n",
    "\n",
    "\n",
    "### Business problem: \n",
    "Predict whether a customer will enroll for a certificate of deposit product at a bank, after one or more phone calls.\n",
    "\n",
    "### Labeled Data: \n",
    "Customer demographics (age, employment, type of job, education etc.), responses to marketing events (including past response), external factors (month, day of the week etc.) and whether the customer is enrolled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Imports\n",
    "\n",
    "Import libraries and define environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer  \n",
    "import random\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Storage Bucket\n",
    "\n",
    "Please note the bucket_name needs to be unique globally for AWS S3, Please enter a first and last name to generate a unique bucket name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique S3 Bucket Name Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_s3_name():\n",
    "    fname = input('Enter your first name, eg. joe :')\n",
    "    lname = input('Enter your last name, eg. smith :')\n",
    "    bucket_name = 'sagemaker-844-'+fname+lname+str(random.randint(100, 999))\n",
    "    print(\"Your unique S3 bucket name will be '{}'\".format(bucket_name))\n",
    "    return bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eg. bucket_name = 'sagemaker-844-firstnamelastname###' <-- example of a unique bucket name \n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "#        s3.create_bucket(Bucket=bucket_name)\n",
    "        bucket_name = unique_s3_name()\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Download Data\n",
    "Download data from external URL to your SageMaker instance, and read it as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "#    urllib.request.urlretrieve (\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
    "    !wget -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
    "    print('Success: downloaded bank-additional.zip!')    \n",
    "    print('')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "    print('')\n",
    "\n",
    "\n",
    "try:    \n",
    "    !unzip -o bank-additional.zip    \n",
    "    print('Success: Unzipped bank-additional.zip to .csv!')    \n",
    "    print('')    \n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "    print('')\n",
    "    \n",
    "try:\n",
    "\n",
    "    raw_data = pd.read_csv('./bank-additional/bank-additional-full.csv', sep=\";\")\n",
    "#    model_data = pd.read_csv('./bank_clean.csv',index_col=0)\n",
    "    print('Success: Data loaded bank-additional-full.csv into dataframe.')\n",
    "    print('')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Initial View the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dateset is downloaded and in a Pandas dataframe we can look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data contains 20 features for each customer. Here is a summary of each column:\n",
    "\n",
    "Demographics:\n",
    "* age: Customer's age (numeric)\n",
    "* job: Type of job (categorical: 'admin.', 'services', ...)\n",
    "* marital: Marital status (categorical: 'married', 'single', ...)\n",
    "* education: Level of education (categorical: 'basic.4y', 'high.school', ...)\n",
    "\n",
    "Past customer events:\n",
    "* default: Has credit in default? (categorical: 'no', 'unknown', ...)\n",
    "* housing: Has housing loan? (categorical: 'no', 'yes', ...)\n",
    "* loan: Has personal loan? (categorical: 'no', 'yes', ...)\n",
    "\n",
    "Past direct marketing contacts:\n",
    "* contact: Contact communication type (categorical: 'cellular', 'telephone', ...)\n",
    "* month: Last contact month of year (categorical: 'may', 'nov', ...)\n",
    "* day_of_week: Last contact day of the week (categorical: 'mon', 'fri', ...)\n",
    "* duration: Last contact duration, in seconds (numeric). Important note: If duration = 0 then y = 'no'.\n",
    "\n",
    "Campaign information:\n",
    "* campaign: Number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "* pdays: Number of days that passed by after the client was last contacted from a previous campaign (numeric)\n",
    "* previous: Number of contacts performed before this campaign and for this client (numeric)\n",
    "* poutcome: Outcome of the previous marketing campaign (categorical: 'nonexistent','success', ...)\n",
    "\n",
    "External environment factors:\n",
    "* emp.var.rate: Employment variation rate - quarterly indicator (numeric)\n",
    "* cons.price.idx: Consumer price index - monthly indicator (numeric)\n",
    "* cons.conf.idx: Consumer confidence index - monthly indicator (numeric)\n",
    "* euribor3m: Euribor 3 month rate - daily indicator (numeric)\n",
    "* nr.employed: Number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Target variable:\n",
    "* y: Has the client subscribed a term deposit? (binary: 'yes','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the Min and Max of the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_colns = ['age', 'duration', 'campaign', 'pdays','previous', 'emp.var.rate', 'cons.price.idx',\n",
    "       'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "for x in numeric_colns:\n",
    "    value = np.array(raw_data[x])\n",
    "    print(x,':', 'min:', np.min(value), 'max:', np.max(value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the range of each numeric coiumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in numeric_colns:\n",
    "    raw_data[x] = raw_data[x].apply(round)\n",
    "    value = np.array(raw_data[x])\n",
    "    print(x,':', 'min:', np.min(value), 'max:', np.max(value), raw_data[x].dtype)\n",
    "    print(raw_data[x].unique())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a couple of Box Plots to show the relation of some categorical columns vs. numeric columns.\n",
    "\n",
    "e.g. \"What is the distribution of ages when someone has 'defaulted' on a loan in the past?\"   OR\n",
    "\n",
    "e.g. \"what is the distribution of ages based on the past success or failure of a past loan?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='default', y='age', data=raw_data)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='poutcome', y='age', data=raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a scatter joint plot to show the relation of some numeric columns vs. numeric columns.\n",
    "\n",
    "e.g. \"What is the Ages vs. the days of a campaign?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "g = sns.jointplot(\"age\", \"campaign\", data=raw_data,\n",
    "                  kind=\"reg\", truncate=False,\n",
    "                  xlim=(40, 100), ylim=(0, 50),\n",
    "                  color=\"m\", height=10)\n",
    "g.set_axis_labels(\"Age (years)\", \"Campaign (days)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(x=\"age\", y=\"campaign\", hue=\"default\",\n",
    "               height=10, data=raw_data)\n",
    "g.set_axis_labels(\"Age (years)\", \"Campaign (days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the pie chart of the distribution of our dataset in terms of the level of education of our customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(raw_data['education'])\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.pie([float(c[v]) for v in c], labels=[str(k) for k in c], autopct=None)\n",
    "plt.title('Level of Education Achieved') \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for a in raw_data.columns[1:5]:\n",
    "    data = raw_data[a].value_counts()\n",
    "    values = raw_data[a].value_counts().index.to_list()\n",
    "    counts = raw_data[a].value_counts().to_list()\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    ax = sns.barplot(x = values, y = counts)\n",
    "    \n",
    "    plt.title(a)\n",
    "    plt.xticks(rotation=45)\n",
    "#    print(a, values, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have explored the data thoroughly, we can now start 'cleaning' the dataset. Let's remind ourselves what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many records have \"999\" for pdays, which is the number of days that passed by after a client was last contacted. It is very likely to be a magic number to represent that no contact was made before. Therefore, we create a new column called \"no_previous_contact\", then make it \"1\" when pdays is 999 and \"0\" otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Indicator variable to capture when pdays takes a value of 999\n",
    "raw_data['no_previous_contact'] = np.where(raw_data['pdays'] == 999, 1, 0) \n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: there is ONE additional column added to the end to give us 22 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the \"job\" column, various categories mean the customer is not working, e.g., \"student\", \"retire\", and \"unemployed\". Since it is highly likely whether or not a customer is working will affect his/her decision to enroll in the certificate of deposit, we create a new column to show whether the customer is working based on the \"job\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicator for individuals not actively employed\n",
    "raw_data['not_working'] = np.where(np.in1d(raw_data['job'], ['student', 'retired', 'unemployed']), 1, 0)  \n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we added another column at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert categorical data to numeric using pd.get_dummies(data), and view the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to sets of indicators\n",
    "model_data = pd.get_dummies(raw_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the dataset now to see the effects of the encoding steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, you can now see we have a total of 67 columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Should we Drop Any Data?\n",
    "Another question to ask yourself before building a model is whether certain features will add value in your final use case. For example, if your goal is to deliver the best prediction, then will you have access to that data at the moment of prediction? Knowing it's raining is highly predictive for umbrella sales, but forecasting weather far enough out to plan inventory on umbrellas is probably just as difficult as forecasting umbrella sales without knowledge of the weather. So, including this in your model may give you a false sense of precision.\n",
    "\n",
    "Certain economic features in the data won't be available at the time of predicting a customer's enrollment behaviour, or they can be as difficult to forecast as the business problem, with data being only available for defined time periods and on a lag.\n",
    "\n",
    "So we remove the economic features and duration from the data as they would need to be forecasted with high precision to use as inputs in future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop these columns if they can't be used in making a prediction\n",
    "model_data = model_data.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View model_data. Now the dataset is cleaned and ready to be split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see we went from 21 columns and expanded it to 61 columns because of the encoding and dropping column process and all numeric values. We are now ready to start splitting up the data for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Shuffle and split the data\n",
    "Shuffle and split the data into training and test sets. In this example, select 70% of customers for training data.\n",
    "\n",
    "The rest 30% of customers data is used to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data))])\n",
    "print(\"We have {} rows of Training data with {} columns\".format(train_data.shape[0], train_data.shape[1]))\n",
    "print('and')\n",
    "print(\"We have {} rows of Testing data with {} columns\".format(test_data.shape[0], test_data.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Training\n",
    "Train the training data using SageMaker pre-built XGBoost model.  \n",
    "\n",
    "XGBoost is a gradient-based optimization to iteratively refine the model parameters. Gradient-based optimization is to find model parameter values that minimize the model error, using the gradient of the model loss function.\n",
    "\n",
    "Reformat the header and first column, load data from S3. \n",
    "\n",
    "(Disregard message on second version SDK v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the 'label' encoded columns from the train_data as that shouldn't be used for training\n",
    "#Save to a .csv file without column names as training won't use these\n",
    "#Concatenation\n",
    "pd.concat([train_data['y_yes'],train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the file to S3 for Amazon SageMaker training to pickup.\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Set up SageMaker session\n",
    "\n",
    "Create a Sagemaker session, an estimator (an instance) of the XGBoost model, and define the model's hyperparameters. \n",
    "\n",
    "(Disregard message on second version SDK v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "xgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m4.xlarge',output_path='s3://{}/{}/output'.format(bucket_name, prefix),sagemaker_session=sess)\n",
    "xgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoost algorithm performs well in machine learning competitions because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. You can use XGBoost for regression, classification (binary and multiclass), and ranking problems.\n",
    "\n",
    "<b> Resource: </b> For a more indepth explanation of xgBoost: https://youtu.be/8b1JEDvenQU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This training can take up to 3 minutes to complete\n",
    "xgb.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11:  Deploy the Model\n",
    "\n",
    "Deploy the model on a server and create an endpoint\n",
    "\n",
    "(Disregard message on second version SDK v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This deployment step can take up to 10 minutes to complete\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1,instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12:  Make Predictions\n",
    "\n",
    "Run the model to create predictions on whether customers in the test data enrolled for the certificate of deposit product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n",
    "xgb_predictor.content_type = 'text/csv' # set the data type for an inference\n",
    "xgb_predictor.serializer = csv_serializer # set the serializer type\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "print(predictions_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13:  Evaluate model performance\n",
    "\n",
    "Compare actual vs. predictions in a confusion matrix. \n",
    "\n",
    "What shows for the overall classification rate%?, What is the precision and recall of the model?\n",
    "\n",
    "Remember, a high Precision or Recall only depends on what application your model is trying to solve.\n",
    "\n",
    "of 65% (278/429) for enrolled and 90% (10,785/11,928) for customers who didn't enroll.\n",
    "\n",
    "Precision = True Positive / (True Positive + False Positive) = True Positive / Total Predicted Positive\n",
    "\n",
    "Recall = True Positive / (True Positive + False Negative) = True Positive / Total Actual Positive = 278/(1143+278) = 0.1956\n",
    "\n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall) = 2*(0.65*0.1956)/(0.65+0.1956) = 0.3005\n",
    "\n",
    "<b>Resource:</b> StatQuest explanation of Confusion Matrix https://youtu.be/j-EB6RqqjGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions_array), rownames=['Observed'], colnames=['Predicted'])\n",
    "tn = cm.iloc[0,0]; fn = cm.iloc[1,0]; tp = cm.iloc[1,1]; fp = cm.iloc[0,1]; p = (tp+tn)/(tp+tn+fp+fn)*100\n",
    "\n",
    "print(\"Evaluate model performance Observed\")\n",
    "print(\"\\n{0:<20}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", p))\n",
    "\n",
    "\n",
    "\n",
    "print(\"CONFUSION MATRIX\")\n",
    "\n",
    "\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\"No Purchase\", tn/(tn+fn)*100,tn, fp/(tp+fp)*100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<}) \\n\".format(\"Purchase\", fn/(tn+fn)*100,fn, tp/(tp+fp)*100, tp))\n",
    "\n",
    "precision = tp/(tp+fp)*100\n",
    "recall = tp/(tp+fn)*100\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Precision: {}%\".format(round(precision,3)))\n",
    "print(\"Recall: {}%\".format(round(recall),3))\n",
    "print(\"F1 Score: {}\".format(round(f1_score)))\n",
    "print(\"\")\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Terminate resources\n",
    "\n",
    "### IMPORTANT STEP!!! Terminate resources not actively being used to reduce costs and is a best practice. Delete endpoint and all objects in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint)\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.all().delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
